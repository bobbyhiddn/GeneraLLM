# Directory: .

/.env: 
```
FLASK_SECRET_KEY="8c027df3021f0fcab5f4e2f2a11e321f971d203446f627de52251893dcd901ec"
HUGGINGFACE_TOKEN="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"
MODEL_NAME="google/gemma-2-2b-it"

```
/.gitignore: 
```
**/.env
**/.cache
**/__pycache__
```
## /app/

/app/main.py: 
```
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import pipeline
import os
import torch
import logging
import dotenv

app = FastAPI()

# Load environment variables from .env file
dotenv.load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
MODEL_NAME = os.getenv("MODEL_NAME")

if not HUGGINGFACE_TOKEN or not MODEL_NAME:
    logger.error("Environment variables HUGGINGFACE_TOKEN and MODEL_NAME must be set.")
    raise Exception("Environment variables HUGGINGFACE_TOKEN and MODEL_NAME must be set.")

# Load the model
try:
    logger.info("Loading model...")
    model = pipeline(
        "text-generation",
        model=MODEL_NAME,
        token=HUGGINGFACE_TOKEN,
        torch_dtype=torch.float32,  # Explicitly set dtype for CPU
        device_map="cpu",           # Ensure model loads on CPU
        trust_remote_code=True      # Add if necessary
    )
    logger.info("Model loaded successfully.")
except Exception as e:
    logger.exception("Model loading failed")
    raise HTTPException(status_code=500, detail=f"Model loading failed: {str(e)}")

# Define the request model
class GenerateRequest(BaseModel):
    prompt: str

@app.post("/generate")
async def generate_text(request: GenerateRequest):
    try:
        prompt = request.prompt
        logger.info(f"Received prompt: {prompt}")
        output = model(prompt, max_length=100)
        generated_text = output[0]["generated_text"]
        logger.info("Text generation successful.")
        return {"generated_text": generated_text}
    except Exception as e:
        logger.exception("Generation failed")
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

@app.get("/")
async def root():
    return {"message": "Hello from FastAPI!"}

```
## /app/__pycache__/

/app/__pycache__/main.cpython-311.pyc: [non-readable or binary content]
/app/__pycache__/main.cpython-39.pyc: [non-readable or binary content]
/flask_keygen.sh: 
```
import secrets
secret_key = secrets.token_hex(32)
print(secret_key)

```
## /image/

/image/.env: 
```
FLASK_SECRET_KEY="8c027df3021f0fcab5f4e2f2a11e321f971d203446f627de52251893dcd901ec"
HUGGINGFACE_TOKEN="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"
MODEL_NAME="google/gemma-2-2b-it"

```
## /image/app/

/image/app/main.py: 
```
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import pipeline
import os
import torch
import logging

app = FastAPI()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
MODEL_NAME = os.getenv("MODEL_NAME")

if not HUGGINGFACE_TOKEN or not MODEL_NAME:
    logger.error("Environment variables HUGGINGFACE_TOKEN and MODEL_NAME must be set.")
    raise Exception("Environment variables HUGGINGFACE_TOKEN and MODEL_NAME must be set.")

# Load the model
try:
    logger.info("Loading model...")
    model = pipeline(
        "text-generation",
        model=MODEL_NAME,
        token=HUGGINGFACE_TOKEN,
        torch_dtype=torch.float32,  # Explicitly set dtype for CPU
        device_map="cpu",           # Ensure model loads on CPU
        trust_remote_code=True      # Add if necessary
    )
    logger.info("Model loaded successfully.")
except Exception as e:
    logger.exception("Model loading failed")
    raise HTTPException(status_code=500, detail=f"Model loading failed: {str(e)}")

# Define the request model
class GenerateRequest(BaseModel):
    prompt: str

@app.post("/generate")
async def generate_text(request: GenerateRequest):
    try:
        prompt = request.prompt
        logger.info(f"Received prompt: {prompt}")
        output = model(prompt, max_length=100)
        generated_text = output[0]["generated_text"]
        logger.info("Text generation successful.")
        return {"generated_text": generated_text}
    except Exception as e:
        logger.exception("Generation failed")
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

@app.get("/")
async def root():
    return {"message": "Hello from FastAPI!"}

```
/image/app/requirements.txt: 
```
fastapi
uvicorn
transformers
torch
accelerate

```
## /image/app/__pycache__/

/image/app/__pycache__/main.cpython-311.pyc: [non-readable or binary content]
/image/docker-compose.yml: 
```
services:
  app:
    build: .
    container_name: generallm  # Set the container name here
    ports:
      - "8000:8000"
    environment:
      - FLASK_SECRET_KEY=${FLASK_SECRET_KEY}
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - MODEL_NAME=${MODEL_NAME}
    volumes:
      - ./app:/app

```
/image/Dockerfile: 
```
# Use an official Python runtime as the base image
FROM python:3.11-slim

# Set the working directory
WORKDIR /app

# Copy requirements and install
COPY app/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the application code
COPY app/ .

# Expose the port
EXPOSE 8000

# Command to run the application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

```
/image/fly_deploy.sh: 
```
#!/bin/bash

# Load .env file
if [ ! -f .env ]; then
  echo ".env file not found!"
  exit 1
fi

# Read the .env file and export variables
export $(grep -v '^#' .env | xargs)

# Set the secrets using flyctl
flyctl secrets set \
  FLASK_SECRET_KEY="$FLASK_SECRET_KEY" \
  HUGGINGFACE_TOKEN="$HUGGINGFACE_TOKEN" \
  MODEL_NAME="$MODEL_NAME"
  # Add any other environment variables here

echo "Secrets set successfully on Fly.io!"

# Deploy to Fly.io
fly deploy

```
/README.md: 
```
# FastAPI LLM Deployment on Fly.io

## Overview

This project is a FastAPI-based web service for deploying a lightweight LLM using the Hugging Face Transformers library. It can be run locally with Docker and deployed to Fly.io for serverless, scalable infrastructure.

## Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/project.git
   cd project

```
